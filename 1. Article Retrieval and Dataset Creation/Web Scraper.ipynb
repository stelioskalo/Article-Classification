{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Web Scraper.ipynb","provenance":[{"file_id":"10tE8BPZ1Kn6UadEB28ZXxelwmH8wzvOn","timestamp":1585658740108}],"collapsed_sections":[],"mount_file_id":"10tE8BPZ1Kn6UadEB28ZXxelwmH8wzvOn","authorship_tag":"ABX9TyORPNFQy/3SNymZzXwfjS/p"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_0Zsiw7M09Ax","colab_type":"code","colab":{}},"source":["import requests \n","from bs4 import BeautifulSoup\n","import csv\n","import pandas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBnIMc9vgGGG","colab_type":"code","outputId":"bde3cc7e-4420-4392-9e3d-b6df43c42516","executionInfo":{"status":"ok","timestamp":1585656211643,"user_tz":-60,"elapsed":1574,"user":{"displayName":"Stelios Kalorkotis","photoUrl":"","userId":"02146774481839036297"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["articles = []\n","category = []\n","\n","with open(\"drive/My Drive/Colab Notebooks/articles3.csv\") as f:\n","    articles = [row.split(',')[0] for row in f]\n","with open(\"drive/My Drive/Colab Notebooks/articles3.csv\") as f:\n","    category = [row.split(',')[1] for row in f]    \n","\n","print(\"Number of Article URLs in Dataset: \" + str(len(articles)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Number of Article URLs in Dataset: 1287\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R7OvCWc31EuQ","colab_type":"code","outputId":"8d3f74c1-301d-428a-d357-d99c40288a50","executionInfo":{"status":"ok","timestamp":1585658375819,"user_tz":-60,"elapsed":1110475,"user":{"displayName":"Stelios Kalorkotis","photoUrl":"","userId":"02146774481839036297"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open('drive/My Drive/Colab Notebooks/articledataset3.csv', 'w') as f:\n","  writer = csv.writer(f)\n","  writer.writerow(['Title', 'Article', \"Category\"])\n","  for i in range(len(articles)):\n","    r1 = requests.get(articles[i])\n","    coverpage = r1.content\n","    article_category = category[i]\n","\n","    soup1 = BeautifulSoup(coverpage, 'html5lib')\n","    paragraphs = soup1.find_all('p')\n","\n","    full_article = \"\"\n","    for i in range(len(paragraphs) - 2):\n","      full_article = full_article + paragraphs[i].getText() + \" \"\n","    writer.writerow([\"title\", full_article, article_category])\n","  print(\"Articles uploaded to CSV file\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Articles uploaded to CSV file\n"],"name":"stdout"}]}]}